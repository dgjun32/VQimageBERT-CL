{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_model import Encoder, Decoder, VQVAE\n",
    "from model import VQImageGPT\n",
    "from loss import VQImageGPTModel\n",
    "from config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset('val', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "lrs = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=0.01, epochs=100, steps_per_epoch=1, pct_start=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009976895657163733]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs.step()\n",
    "lrs.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "image = torch.rand(4, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQImageGPT(**cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = VQImageGPTModel(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQImageGPT(\n",
       "  (vqvae): VQVAE(\n",
       "    (encoder): Encoder(\n",
       "      (conv_stack): Sequential(\n",
       "        (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ResidualStack(\n",
       "          (stack): ModuleList(\n",
       "            (0): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_quantization_conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (vector_quantization): VectorQuantizer(\n",
       "      (embedding): Embedding(8092, 768)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (inverse_conv_stack): Sequential(\n",
       "        (0): ConvTranspose2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ResidualStack(\n",
       "          (stack): ModuleList(\n",
       "            (0): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvTranspose2d(768, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): ConvTranspose2d(384, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): ConvTranspose2d(192, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): ConvTranspose2d(96, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (9): ConvTranspose2d(48, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss.vq_imagegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9726, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a06f093cbaeec228923b48b78a81a3ad5fd7f429a83b09002ee2923fc0db60a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('imagegpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
