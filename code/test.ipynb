{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_model import Encoder, Decoder, VQVAE\n",
    "from model import VQImageGPT\n",
    "from loss import VQImageGPTModel\n",
    "from config import cfg\n",
    "from datasets import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqvae import VQVAE as VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VQ(channel=512, n_res_block=0,\n",
    "        n_res_channel=32, embed_dim=256,\n",
    "        n_embed=8192, stride=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/Users/dongdori/workspace/codespace/projects/vqvae_imagebert/model/vqvae_hard_biggerset_011.pt',\n",
    "map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loaded = dict()\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    new_loaded[key.replace(\"module.\", \"\")] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq.load_state_dict(new_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vq.encode(torch.rand(4,3,196,196))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 24, 24])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset('val', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "image = torch.rand(4, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQImageGPT(**cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = VQImageGPTModel(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQImageGPT(\n",
       "  (vqvae): VQVAE(\n",
       "    (encoder): Encoder(\n",
       "      (conv_stack): Sequential(\n",
       "        (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ResidualStack(\n",
       "          (stack): ModuleList(\n",
       "            (0): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_quantization_conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (vector_quantization): VectorQuantizer(\n",
       "      (embedding): Embedding(8092, 768)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (inverse_conv_stack): Sequential(\n",
       "        (0): ConvTranspose2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ResidualStack(\n",
       "          (stack): ModuleList(\n",
       "            (0): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ResidualLayer(\n",
       "              (res_block): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvTranspose2d(768, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): ConvTranspose2d(384, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): ConvTranspose2d(192, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): ConvTranspose2d(96, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (9): ConvTranspose2d(48, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (ln_1): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((48, 768), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): PositionwiseFFN(\n",
       "        (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss.vq_imagegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9726, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a06f093cbaeec228923b48b78a81a3ad5fd7f429a83b09002ee2923fc0db60a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('imagegpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
